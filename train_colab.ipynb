{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Credit Card Detection Training - YOLOv8\n",
        "\n",
        "This notebook trains a YOLOv8 model for credit card detection with progressive occlusion evaluation.\n",
        "\n",
        "Features:\n",
        "- Weights & Biases integration for experiment tracking\n",
        "- Google Drive integration for saving models\n",
        "- Progressive occlusion evaluation\n",
        "- Model size: Medium, 100 epochs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Setup Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q ultralytics roboflow python-dotenv opencv-python matplotlib numpy pandas pyyaml wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone or update your repository\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "REPO_DIR = \"/content/credit_card_yolov12\"\n",
        "\n",
        "if Path(REPO_DIR).exists():\n",
        "    print(\"Repository exists, pulling latest changes...\")\n",
        "    %cd {REPO_DIR}\n",
        "    !git pull origin main\n",
        "else:\n",
        "    print(\"Cloning repository...\")\n",
        "    !git clone https://github.com/Turje/credit_card_yolov12.git\n",
        "    %cd credit_card_yolov12\n",
        "\n",
        "# Verify files exist\n",
        "print(\"\\nVerifying required files...\")\n",
        "required_files = [\n",
        "    \"src/split_dataset.py\",\n",
        "    \"src/prepare_progressive_tests.py\",\n",
        "    \"src/train.py\",\n",
        "    \"src/evaluate_progressive.py\"\n",
        "]\n",
        "\n",
        "for file in required_files:\n",
        "    file_path = Path(REPO_DIR) / file\n",
        "    if file_path.exists():\n",
        "        print(f\"âœ… {file}\")\n",
        "    else:\n",
        "        print(f\"âŒ {file} - NOT FOUND!\")\n",
        "\n",
        "# Or if you've uploaded files to Drive, uncomment:\n",
        "# %cd /content/drive/MyDrive/credit_card_yolov12\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Setup Weights & Biases\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "# Login to Weights & Biases\n",
        "wandb.login(key='6defa0781045a6f791ddd5b18bd7ebbdcdfdc86d')\n",
        "\n",
        "# Initialize wandb project\n",
        "wandb.init(\n",
        "    project=\"credit-card-detection\",\n",
        "    name=\"yolov8-medium-100epochs\",\n",
        "    config={\n",
        "        \"model_size\": \"m\",\n",
        "        \"epochs\": 100,\n",
        "        \"imgsz\": 640,\n",
        "        \"batch\": 16,\n",
        "        \"dataset\": \"credit-cards-coco\"\n",
        "    }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option A: Download Dataset from Roboflow (Recommended)\n",
        "\n",
        "**Run this cell to download the dataset directly from Roboflow to Google Drive.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download dataset from Roboflow\n",
        "from roboflow import Roboflow\n",
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "# Your Roboflow API key\n",
        "ROBOFLOW_API_KEY = \"uMvNj8tWIWmY9bXNwrf1\"\n",
        "\n",
        "# Initialize Roboflow\n",
        "rf = Roboflow(api_key=ROBOFLOW_API_KEY)\n",
        "\n",
        "# Download credit card dataset\n",
        "project = rf.workspace(\"efe-efesefe-gvfaz\").project(\"credit-cards-n4hrw\")\n",
        "version = project.version(3)\n",
        "\n",
        "# Strategy: Download to local /content first (faster), then move to Drive\n",
        "LOCAL_DOWNLOAD_DIR = \"/content/datasets_temp\"\n",
        "DRIVE_DATASET_DIR = \"/content/drive/MyDrive/credit_card_yolov12/datasets\"\n",
        "\n",
        "# Create directories\n",
        "Path(LOCAL_DOWNLOAD_DIR).mkdir(parents=True, exist_ok=True)\n",
        "Path(DRIVE_DATASET_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Change to download directory (Roboflow sometimes downloads to current directory)\n",
        "original_cwd = os.getcwd()\n",
        "os.chdir(LOCAL_DOWNLOAD_DIR)\n",
        "\n",
        "print(\"Downloading dataset from Roboflow...\")\n",
        "print(f\"Current directory: {os.getcwd()}\")\n",
        "print(f\"Download location: {LOCAL_DOWNLOAD_DIR}\")\n",
        "\n",
        "# Download in COCO format to local storage first\n",
        "try:\n",
        "    print(\"Starting download...\")\n",
        "    # Try downloading without location first to see where it goes\n",
        "    dataset = version.download(\"coco\")\n",
        "    print(f\"âœ… Download completed!\")\n",
        "    print(f\"Roboflow returned location: {dataset.location}\")\n",
        "    \n",
        "    # Wait a moment for files to be written\n",
        "    time.sleep(2)\n",
        "    \n",
        "    # Check what Roboflow actually created\n",
        "    print(f\"\\nChecking download directory contents...\")\n",
        "    download_path = Path(dataset.location) if isinstance(dataset.location, str) else dataset.location\n",
        "    \n",
        "    print(f\"Roboflow location: {download_path}\")\n",
        "    print(f\"Location exists: {download_path.exists()}\")\n",
        "    \n",
        "    if download_path.exists():\n",
        "        items = list(download_path.iterdir())\n",
        "        print(f\"Items in {download_path}: {len(items)}\")\n",
        "        for item in items:\n",
        "            size_info = \"\"\n",
        "            if item.is_file():\n",
        "                try:\n",
        "                    size_info = f\" ({item.stat().st_size} bytes)\"\n",
        "                except:\n",
        "                    pass\n",
        "            print(f\"  - {item.name} ({'dir' if item.is_dir() else 'file'}){size_info}\")\n",
        "    \n",
        "    # Also check current directory (Roboflow might download there)\n",
        "    cwd = Path(os.getcwd())\n",
        "    if cwd.exists():\n",
        "        cwd_items = list(cwd.iterdir())\n",
        "        if cwd_items:\n",
        "            print(f\"\\nItems in current directory ({cwd}): {len(cwd_items)}\")\n",
        "            for item in cwd_items[:10]:  # Limit to first 10\n",
        "                print(f\"  - {item.name} ({'dir' if item.is_dir() else 'file'})\")\n",
        "    \n",
        "    # Check parent directory too\n",
        "    parent = download_path.parent\n",
        "    if parent.exists() and parent != download_path:\n",
        "        parent_items = list(parent.iterdir())\n",
        "        print(f\"\\nItems in parent directory ({parent}): {len(parent_items)}\")\n",
        "        for item in parent_items[:10]:\n",
        "            if item.is_dir() and \"credit\" in item.name.lower():\n",
        "                print(f\"  ğŸ“ {item.name}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error during download: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    \n",
        "    # Try alternative: download to current directory\n",
        "    print(\"\\nTrying alternative download method...\")\n",
        "    try:\n",
        "        os.chdir(\"/content\")\n",
        "        dataset = version.download(\"coco\")\n",
        "        print(f\"Alternative download location: {dataset.location}\")\n",
        "    except Exception as e2:\n",
        "        print(f\"Alternative method also failed: {e2}\")\n",
        "        raise\n",
        "finally:\n",
        "    os.chdir(original_cwd)\n",
        "\n",
        "# Find the actual dataset folder\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Locating downloaded dataset...\")\n",
        "\n",
        "# Roboflow might return a string path or a Path object\n",
        "if hasattr(dataset, 'location'):\n",
        "    download_path = Path(dataset.location) if isinstance(dataset.location, str) else dataset.location\n",
        "else:\n",
        "    download_path = Path(LOCAL_DOWNLOAD_DIR)\n",
        "\n",
        "print(f\"Roboflow location: {download_path}\")\n",
        "print(f\"Checking: {download_path}\")\n",
        "print(f\"Exists: {download_path.exists()}\")\n",
        "\n",
        "# Roboflow typically creates a folder named like: credit-cards-n4hrw-3\n",
        "# Let's search more thoroughly\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Searching for dataset folder...\")\n",
        "\n",
        "# Check multiple possible locations\n",
        "search_paths = [\n",
        "    Path(LOCAL_DOWNLOAD_DIR),\n",
        "    Path(\"/content\"),\n",
        "    Path(\"/content/datasets_temp\"),\n",
        "    download_path,\n",
        "]\n",
        "\n",
        "# Also check for common Roboflow folder names\n",
        "possible_names = [\n",
        "    \"credit-cards-n4hrw-3\",\n",
        "    \"credit-cards-n4hrw\",\n",
        "    \"credit-cards\",\n",
        "    \"Credit-Cards-3\",\n",
        "    \"Credit-Cards\"\n",
        "]\n",
        "\n",
        "actual_dataset = None\n",
        "\n",
        "# First, search in all search paths\n",
        "for search_path in search_paths:\n",
        "    if not search_path.exists():\n",
        "        continue\n",
        "    \n",
        "    print(f\"\\nSearching in: {search_path}\")\n",
        "    \n",
        "    # Check if train folder exists directly\n",
        "    if (search_path / \"train\").exists():\n",
        "        actual_dataset = search_path\n",
        "        print(f\"  âœ… Found train folder directly!\")\n",
        "        break\n",
        "    \n",
        "    # Check subdirectories\n",
        "    try:\n",
        "        items = list(search_path.iterdir())\n",
        "        for item in items:\n",
        "            if item.is_dir():\n",
        "                # Check if this folder has train\n",
        "                if (item / \"train\").exists():\n",
        "                    actual_dataset = item\n",
        "                    print(f\"  âœ… Found train folder in: {item.name}\")\n",
        "                    break\n",
        "                \n",
        "                # Check nested structure\n",
        "                try:\n",
        "                    for subitem in item.iterdir():\n",
        "                        if subitem.is_dir() and (subitem / \"train\").exists():\n",
        "                            actual_dataset = subitem\n",
        "                            print(f\"  âœ… Found train folder in nested: {item.name}/{subitem.name}\")\n",
        "                            break\n",
        "                except PermissionError:\n",
        "                    pass\n",
        "                \n",
        "                if actual_dataset:\n",
        "                    break\n",
        "    except PermissionError:\n",
        "        print(f\"  âš ï¸ Permission denied\")\n",
        "        continue\n",
        "    \n",
        "    if actual_dataset:\n",
        "        break\n",
        "\n",
        "# If still not found, try looking for folders with the expected names\n",
        "if not actual_dataset:\n",
        "    print(f\"\\nTrying specific folder names...\")\n",
        "    for search_path in search_paths:\n",
        "        if not search_path.exists():\n",
        "            continue\n",
        "        for name in possible_names:\n",
        "            test_path = search_path / name\n",
        "            if test_path.exists() and test_path.is_dir():\n",
        "                print(f\"  Found folder: {test_path}\")\n",
        "                if (test_path / \"train\").exists():\n",
        "                    actual_dataset = test_path\n",
        "                    print(f\"    âœ… Has train folder!\")\n",
        "                    break\n",
        "                # Check nested\n",
        "                try:\n",
        "                    for subitem in test_path.iterdir():\n",
        "                        if subitem.is_dir() and (subitem / \"train\").exists():\n",
        "                            actual_dataset = subitem\n",
        "                            print(f\"    âœ… Found train in nested: {subitem.name}\")\n",
        "                            break\n",
        "                except:\n",
        "                    pass\n",
        "            if actual_dataset:\n",
        "                break\n",
        "        if actual_dataset:\n",
        "            break\n",
        "\n",
        "# If found, copy to Drive\n",
        "if actual_dataset:\n",
        "    source_path = Path(actual_dataset)\n",
        "    dest_path = Path(DRIVE_DATASET_DIR) / source_path.name\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"âœ… Dataset found at: {source_path}\")\n",
        "    print(f\"Copying to Drive: {dest_path}\")\n",
        "    \n",
        "    # Copy to Drive\n",
        "    if dest_path.exists():\n",
        "        print(f\"âš ï¸ Destination exists, removing old version...\")\n",
        "        shutil.rmtree(dest_path)\n",
        "    \n",
        "    shutil.copytree(source_path, dest_path)\n",
        "    print(f\"âœ… Dataset copied to Drive!\")\n",
        "    \n",
        "    ORIGINAL_DATASET = str(dest_path)\n",
        "    \n",
        "    # Clean up local temp\n",
        "    print(f\"Cleaning up temporary files...\")\n",
        "    shutil.rmtree(LOCAL_DOWNLOAD_DIR, ignore_errors=True)\n",
        "    \n",
        "else:\n",
        "    print(f\"\\nâŒ Could not find dataset folder with 'train' subdirectory\")\n",
        "    print(f\"Please check manually:\")\n",
        "    print(f\"  - {download_path}\")\n",
        "    print(f\"  - {LOCAL_DOWNLOAD_DIR}\")\n",
        "    print(f\"  - /content\")\n",
        "    ORIGINAL_DATASET = str(Path(DRIVE_DATASET_DIR) / \"credit-cards-n4hrw-3\")\n",
        "\n",
        "# Verify final dataset\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Final dataset path: {ORIGINAL_DATASET}\")\n",
        "print(f\"Path exists: {Path(ORIGINAL_DATASET).exists()}\")\n",
        "\n",
        "if Path(ORIGINAL_DATASET).exists():\n",
        "    train_path = Path(ORIGINAL_DATASET) / \"train\"\n",
        "    if train_path.exists():\n",
        "        train_files = list(train_path.glob(\"*.json\"))\n",
        "        train_images = list(train_path.glob(\"*.jpg\")) + list(train_path.glob(\"*.png\"))\n",
        "        print(f\"\\nâœ… Dataset verified!\")\n",
        "        print(f\"   - COCO annotations: {len(train_files)}\")\n",
        "        print(f\"   - Images: {len(train_images)}\")\n",
        "    else:\n",
        "        print(f\"\\nâš ï¸ Train folder not found at: {train_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option B: Use Dataset Already in Drive\n",
        "\n",
        "**Skip this section if you ran Option A above.**\n",
        "\n",
        "If you already have the dataset uploaded to Google Drive, skip Option A and make sure it's in:\n",
        "`/content/drive/MyDrive/credit_card_yolov12/datasets/`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Diagnostic: Check Dataset Structure\n",
        "\n",
        "If the dataset wasn't found, run this cell to see what's actually in the download directory:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Diagnostic: List all folders and files in the datasets directory\n",
        "from pathlib import Path\n",
        "\n",
        "DATASET_BASE = \"/content/drive/MyDrive/credit_card_yolov12/datasets\"\n",
        "\n",
        "def list_directory(path, max_depth=3, current_depth=0, prefix=\"\"):\n",
        "    \"\"\"Recursively list directory structure.\"\"\"\n",
        "    if current_depth > max_depth:\n",
        "        return\n",
        "    \n",
        "    try:\n",
        "        path_obj = Path(path)\n",
        "        if not path_obj.exists():\n",
        "            print(f\"{prefix}âŒ Path does not exist: {path}\")\n",
        "            return\n",
        "        \n",
        "        items = sorted(path_obj.iterdir())\n",
        "        for i, item in enumerate(items):\n",
        "            is_last = i == len(items) - 1\n",
        "            current_prefix = \"â””â”€â”€ \" if is_last else \"â”œâ”€â”€ \"\n",
        "            print(f\"{prefix}{current_prefix}{item.name}\")\n",
        "            \n",
        "            if item.is_dir() and current_depth < max_depth:\n",
        "                next_prefix = prefix + (\"    \" if is_last else \"â”‚   \")\n",
        "                list_directory(item, max_depth, current_depth + 1, next_prefix)\n",
        "    except Exception as e:\n",
        "        print(f\"{prefix}âŒ Error: {e}\")\n",
        "\n",
        "print(\"Dataset directory structure:\")\n",
        "print(\"=\" * 60)\n",
        "list_directory(DATASET_BASE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Prepare Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Add src to path\n",
        "sys.path.insert(0, '/content/credit_card_yolov12')\n",
        "\n",
        "# Set dataset paths\n",
        "DATASET_BASE = \"/content/drive/MyDrive/credit_card_yolov12/datasets\"\n",
        "\n",
        "# Check if ORIGINAL_DATASET was set in Option A\n",
        "if 'ORIGINAL_DATASET' not in globals():\n",
        "    # Find the downloaded dataset folder (Roboflow creates folders like \"credit-cards-n4hrw-3\")\n",
        "    dataset_folders = list(Path(DATASET_BASE).glob(\"credit-cards*\"))\n",
        "    \n",
        "    # Look for folder with train subdirectory (COCO format)\n",
        "    actual_dataset = None\n",
        "    for folder in dataset_folders:\n",
        "        if folder.is_dir():\n",
        "            # Check if it has train folder or is the dataset root\n",
        "            if (folder / \"train\").exists():\n",
        "                actual_dataset = folder\n",
        "                break\n",
        "            # Also check subdirectories\n",
        "            for subfolder in folder.iterdir():\n",
        "                if subfolder.is_dir() and (subfolder / \"train\").exists():\n",
        "                    actual_dataset = subfolder\n",
        "                    break\n",
        "            if actual_dataset:\n",
        "                break\n",
        "    \n",
        "    if actual_dataset:\n",
        "        ORIGINAL_DATASET = str(actual_dataset)\n",
        "        print(f\"âœ… Found dataset: {ORIGINAL_DATASET}\")\n",
        "    else:\n",
        "        # Fallback: try common names\n",
        "        fallback_paths = [\n",
        "            f\"{DATASET_BASE}/credit-cards-n4hrw-3\",\n",
        "            f\"{DATASET_BASE}/credit-cards-coco\",\n",
        "            f\"{DATASET_BASE}/credit-cards-n4hrw-3/credit-cards-n4hrw-3\"\n",
        "        ]\n",
        "        for path in fallback_paths:\n",
        "            if Path(path).exists():\n",
        "                ORIGINAL_DATASET = path\n",
        "                print(f\"âœ… Found dataset (fallback): {ORIGINAL_DATASET}\")\n",
        "                break\n",
        "        else:\n",
        "            ORIGINAL_DATASET = fallback_paths[0]\n",
        "            print(f\"âš ï¸ Using fallback path: {ORIGINAL_DATASET}\")\n",
        "\n",
        "print(f\"\\nDataset location: {ORIGINAL_DATASET}\")\n",
        "print(f\"Dataset exists: {Path(ORIGINAL_DATASET).exists()}\")\n",
        "\n",
        "# List contents to verify\n",
        "if Path(ORIGINAL_DATASET).exists():\n",
        "    print(f\"\\nDataset contents:\")\n",
        "    for item in sorted(Path(ORIGINAL_DATASET).iterdir()):\n",
        "        item_type = \"ğŸ“\" if item.is_dir() else \"ğŸ“„\"\n",
        "        print(f\"  {item_type} {item.name}\")\n",
        "    \n",
        "    # Check for train folder\n",
        "    train_path = Path(ORIGINAL_DATASET) / \"train\"\n",
        "    if train_path.exists():\n",
        "        print(f\"\\nâœ… Train folder found!\")\n",
        "        train_files = list(train_path.glob(\"*.json\"))\n",
        "        train_images = list(train_path.glob(\"*.jpg\")) + list(train_path.glob(\"*.png\"))\n",
        "        print(f\"   - Annotations: {len(train_files)}\")\n",
        "        print(f\"   - Images: {len(train_images)}\")\n",
        "    else:\n",
        "        print(f\"\\nâš ï¸ Train folder not found in {ORIGINAL_DATASET}\")\n",
        "        print(f\"   Please check the dataset structure\")\n",
        "else:\n",
        "    print(f\"\\nâŒ Dataset not found at {ORIGINAL_DATASET}\")\n",
        "    print(f\"   Available folders in {DATASET_BASE}:\")\n",
        "    if Path(DATASET_BASE).exists():\n",
        "        for item in Path(DATASET_BASE).iterdir():\n",
        "            print(f\"     - {item.name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split dataset\n",
        "!python src/split_dataset.py --dataset {ORIGINAL_DATASET} --seed 42\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate progressive occlusion test sets\n",
        "TEST_DATASET = f\"{DATASET_BASE}/credit-cards-coco_split/test\"\n",
        "!python src/prepare_progressive_tests.py --test-dataset {TEST_DATASET} --type crop --seed 42\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Train Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "import yaml\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Model configuration\n",
        "MODEL_SIZE = \"m\"  # n, s, m, l, x\n",
        "EPOCHS = 100\n",
        "IMG_SIZE = 640\n",
        "BATCH = 16\n",
        "\n",
        "# Dataset paths\n",
        "TRAIN_DATASET = f\"{DATASET_BASE}/credit-cards-coco_split/train\"\n",
        "VAL_DATASET = f\"{DATASET_BASE}/credit-cards-coco_split/val\"\n",
        "\n",
        "print(f\"Training on: {TRAIN_DATASET}\")\n",
        "print(f\"Validating on: {VAL_DATASET}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare dataset config for YOLOv8\n",
        "def prepare_yolo_config(train_path, val_path, output_config=\"dataset.yaml\"):\n",
        "    \"\"\"Create YOLOv8 dataset config.\"\"\"\n",
        "    # Load COCO to get class names\n",
        "    ann_file = Path(train_path) / \"train\" / \"_annotations.coco.json\"\n",
        "    with open(ann_file, 'r') as f:\n",
        "        coco_data = json.load(f)\n",
        "    \n",
        "    categories = coco_data.get('categories', [])\n",
        "    class_names = [cat['name'] for cat in sorted(categories, key=lambda x: x['id'])]\n",
        "    \n",
        "    # Create YOLOv8 config\n",
        "    config = {\n",
        "        'path': str(Path(train_path).parent.absolute()),\n",
        "        'train': 'train',\n",
        "        'val': 'val',\n",
        "        'names': {i: name for i, name in enumerate(class_names)},\n",
        "        'nc': len(class_names)\n",
        "    }\n",
        "    \n",
        "    with open(output_config, 'w') as f:\n",
        "        yaml.dump(config, f, default_flow_style=False)\n",
        "    \n",
        "    print(f\"Dataset config created: {output_config}\")\n",
        "    print(f\"Classes: {class_names}\")\n",
        "    return output_config\n",
        "\n",
        "config_file = prepare_yolo_config(TRAIN_DATASET, VAL_DATASET)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert COCO to YOLO format\n",
        "from src.train import convert_coco_to_yolo\n",
        "\n",
        "# Convert train set\n",
        "convert_coco_to_yolo(TRAIN_DATASET)\n",
        "\n",
        "# Convert val set  \n",
        "convert_coco_to_yolo(VAL_DATASET)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Save Model to Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize model\n",
        "model = YOLO(f\"yolov8{MODEL_SIZE}.pt\")  # Load pretrained\n",
        "\n",
        "# Train with wandb integration\n",
        "results = model.train(\n",
        "    data=config_file,\n",
        "    epochs=EPOCHS,\n",
        "    imgsz=IMG_SIZE,\n",
        "    batch=BATCH,\n",
        "    project=\"/content/drive/MyDrive/credit_card_yolov12/models\",\n",
        "    name=f\"credit_card_{MODEL_SIZE}\",\n",
        "    exist_ok=True,\n",
        "    save=True,\n",
        "    plots=True,\n",
        "    val=True,\n",
        "    # Weights & Biases integration\n",
        "    project_wandb=\"credit-card-detection\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "# Paths\n",
        "MODEL_DIR = results.save_dir\n",
        "DRIVE_MODEL_DIR = f\"/content/drive/MyDrive/credit_card_yolov12/models/credit_card_{MODEL_SIZE}\"\n",
        "\n",
        "# Copy best model to Drive\n",
        "best_model = Path(MODEL_DIR) / \"weights\" / \"best.pt\"\n",
        "last_model = Path(MODEL_DIR) / \"weights\" / \"last.pt\"\n",
        "\n",
        "Path(DRIVE_MODEL_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "if best_model.exists():\n",
        "    shutil.copy2(best_model, f\"{DRIVE_MODEL_DIR}/best.pt\")\n",
        "    print(f\"âœ… Best model saved to: {DRIVE_MODEL_DIR}/best.pt\")\n",
        "\n",
        "if last_model.exists():\n",
        "    shutil.copy2(last_model, f\"{DRIVE_MODEL_DIR}/last.pt\")\n",
        "    print(f\"âœ… Last model saved to: {DRIVE_MODEL_DIR}/last.pt\")\n",
        "\n",
        "# Copy entire training results\n",
        "shutil.copytree(MODEL_DIR, DRIVE_MODEL_DIR, dirs_exist_ok=True)\n",
        "print(f\"âœ… All training results saved to: {DRIVE_MODEL_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Evaluate on Progressive Occlusion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on progressive occlusion test sets\n",
        "BEST_MODEL = f\"{DRIVE_MODEL_DIR}/best.pt\"\n",
        "TEST_SETS_BASE = f\"{DATASET_BASE}/credit-cards-coco_split\"\n",
        "\n",
        "!python src/evaluate_progressive.py \\\n",
        "    --model {BEST_MODEL} \\\n",
        "    --test-sets {TEST_SETS_BASE} \\\n",
        "    --output /content/drive/MyDrive/credit_card_yolov12/outputs/progressive_evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Log Results to WandB\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Load progressive evaluation results\n",
        "results_file = \"/content/drive/MyDrive/credit_card_yolov12/outputs/progressive_evaluation/progressive_results.json\"\n",
        "\n",
        "if Path(results_file).exists():\n",
        "    with open(results_file, 'r') as f:\n",
        "        eval_results = json.load(f)\n",
        "    \n",
        "    # Log to wandb\n",
        "    for occlusion_level, metrics in eval_results.items():\n",
        "        wandb.log({\n",
        "            f\"mAP50_occlusion_{occlusion_level}\": metrics['mAP50'],\n",
        "            f\"mAP50_95_occlusion_{occlusion_level}\": metrics['mAP50_95'],\n",
        "            f\"precision_occlusion_{occlusion_level}\": metrics['precision'],\n",
        "            f\"recall_occlusion_{occlusion_level}\": metrics['recall'],\n",
        "            f\"f1_occlusion_{occlusion_level}\": metrics['f1']\n",
        "        })\n",
        "    \n",
        "    # Log visualization\n",
        "    plot_file = \"/content/drive/MyDrive/credit_card_yolov12/outputs/progressive_evaluation/progressive_occlusion_results.png\"\n",
        "    if Path(plot_file).exists():\n",
        "        wandb.log({\"progressive_occlusion_plot\": wandb.Image(plot_file)})\n",
        "    \n",
        "    print(\"âœ… Results logged to Weights & Biases\")\n",
        "    print(\"\\nProgressive Occlusion Results:\")\n",
        "    df = pd.DataFrame(eval_results).T\n",
        "    print(df)\n",
        "else:\n",
        "    print(\"âš ï¸ Results file not found\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Finish wandb run\n",
        "wandb.finish()\n",
        "print(\"âœ… Training complete! Check your Weights & Biases dashboard.\")\n",
        "print(f\"âœ… Model saved to: {DRIVE_MODEL_DIR}\")\n",
        "print(f\"âœ… Results saved to: /content/drive/MyDrive/credit_card_yolov12/outputs/progressive_evaluation\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
