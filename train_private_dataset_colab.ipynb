{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Private Dataset Training - YOLOv8\n",
        "\n",
        "This notebook trains a YOLOv8 model for private object detection with 16 categories.\n",
        "\n",
        "Features:\n",
        "- Uses pretrained weights from non-private dataset\n",
        "- 16 categories\n",
        "- Progressive occlusion evaluation (crop type)\n",
        "- Weights & Biases integration\n",
        "- Google Drive integration\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Setup Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "%pip install -q ultralytics roboflow python-dotenv opencv-python matplotlib numpy pandas pyyaml wandb gdown\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone or update your repository\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "REPO_DIR = \"/content/credit_card_yolov12\"\n",
        "\n",
        "if Path(REPO_DIR).exists():\n",
        "    print(\"Repository exists, pulling latest changes...\")\n",
        "    %cd {REPO_DIR}\n",
        "    !git pull origin main\n",
        "else:\n",
        "    print(\"Cloning repository...\")\n",
        "    !git clone https://github.com/Turje/credit_card_yolov12.git\n",
        "    %cd credit_card_yolov12\n",
        "\n",
        "# Verify files exist\n",
        "print(\"\\nVerifying required files...\")\n",
        "required_files = [\n",
        "    \"src/split_dataset.py\",\n",
        "    \"src/prepare_progressive_tests.py\",\n",
        "    \"src/train.py\",\n",
        "    \"src/evaluate_progressive.py\"\n",
        "]\n",
        "\n",
        "for file in required_files:\n",
        "    file_path = Path(REPO_DIR) / file\n",
        "    if file_path.exists():\n",
        "        print(f\"‚úÖ {file}\")\n",
        "    else:\n",
        "        print(f\"‚ùå {file} - NOT FOUND!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Setup Weights & Biases\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "# Login to Weights & Biases\n",
        "wandb.login(key='6defa0781045a6f791ddd5b18bd7ebbdcdfdc86d')\n",
        "\n",
        "# Initialize wandb project\n",
        "wandb.init(\n",
        "    project=\"private-object-detection\",\n",
        "    name=\"yolov8-16categories-pretrained\",\n",
        "    config={\n",
        "        \"model_size\": \"m\",\n",
        "        \"epochs\": 100,\n",
        "        \"imgsz\": 640,\n",
        "        \"batch\": 16,\n",
        "        \"num_classes\": 16,\n",
        "        \"pretrained_weights\": \"/content/drive/MyDrive/yolov12_runs/nonprivate/checkpoints/best.pt\",\n",
        "        \"occlusion_type\": \"crop\"\n",
        "    }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Step 3: Set Dataset Paths (Skip if datasets already downloaded)\n",
        "\n",
        "# If you already have datasets on Drive, set paths here and skip Cell 8\n",
        "# Otherwise, run Cell 8 to download and merge datasets\n",
        "\n",
        "USE_EXISTING_DATASETS = True  # Set to True if datasets are already on Drive\n",
        "\n",
        "if USE_EXISTING_DATASETS:\n",
        "    from pathlib import Path\n",
        "    \n",
        "    DATASET_BASE = \"/content/drive/MyDrive/credit_card_yolov12/datasets/private_dataset\"\n",
        "    \n",
        "    # Find merged dataset (for training)\n",
        "    merged_path = Path(DATASET_BASE) / \"merged_dataset\"\n",
        "    if merged_path.exists() and (merged_path / \"train\" / \"_annotations.coco.json\").exists():\n",
        "        ORIGINAL_DATASET = str(merged_path)\n",
        "        print(f\"‚úÖ Found merged dataset: {ORIGINAL_DATASET}\")\n",
        "    else:\n",
        "        # Find individual datasets and merge them\n",
        "        print(\"‚ö†Ô∏è Merged dataset not found. Looking for individual datasets...\")\n",
        "        \n",
        "        valid_folders = []\n",
        "        for folder in Path(DATASET_BASE).iterdir():\n",
        "            if folder.is_dir() and folder.name not in {'__MACOSX', '.DS_Store'}:\n",
        "                train_check = folder / \"train\" / \"_annotations.coco.json\"\n",
        "                if train_check.exists():\n",
        "                    valid_folders.append(folder)\n",
        "                    print(f\"   ‚úÖ Found: {folder.name}\")\n",
        "        \n",
        "        if len(valid_folders) >= 3:\n",
        "            # Merge datasets\n",
        "            print(f\"\\nüîÑ Merging {len(valid_folders)} datasets...\")\n",
        "            import shutil\n",
        "            import json\n",
        "            \n",
        "            merged_dir = Path(DATASET_BASE) / \"merged_dataset\"\n",
        "            merged_dir.mkdir(exist_ok=True)\n",
        "            merged_train = merged_dir / \"train\"\n",
        "            merged_train.mkdir(exist_ok=True)\n",
        "            \n",
        "            all_images = []\n",
        "            all_annotations = []\n",
        "            image_id_offset = 0\n",
        "            ann_id_offset = 0\n",
        "            categories_map = {}\n",
        "            \n",
        "            for folder_path in valid_folders:\n",
        "                train_folder = folder_path / \"train\"\n",
        "                ann_file = train_folder / \"_annotations.coco.json\"\n",
        "                \n",
        "                if ann_file.exists():\n",
        "                    with open(ann_file, 'r') as f:\n",
        "                        coco_data = json.load(f)\n",
        "                    \n",
        "                    if not categories_map:\n",
        "                        categories_map = {cat['id']: cat for cat in coco_data.get('categories', [])}\n",
        "                    \n",
        "                    for img in coco_data.get('images', []):\n",
        "                        img['id'] = image_id_offset + img['id']\n",
        "                        src_img = train_folder / img['file_name']\n",
        "                        if src_img.exists():\n",
        "                            dst_img = merged_train / img['file_name']\n",
        "                            if dst_img.exists():\n",
        "                                stem = Path(img['file_name']).stem\n",
        "                                ext = Path(img['file_name']).suffix\n",
        "                                img['file_name'] = f\"{folder_path.name}_{stem}{ext}\"\n",
        "                                dst_img = merged_train / img['file_name']\n",
        "                            shutil.copy2(src_img, dst_img)\n",
        "                            all_images.append(img)\n",
        "                    \n",
        "                    for ann in coco_data.get('annotations', []):\n",
        "                        ann['id'] = ann_id_offset + ann['id']\n",
        "                        ann['image_id'] = image_id_offset + ann['image_id']\n",
        "                        all_annotations.append(ann)\n",
        "                    \n",
        "                    image_id_offset = max(img['id'] for img in all_images) + 1\n",
        "                    ann_id_offset = max(ann['id'] for ann in all_annotations) + 1\n",
        "            \n",
        "            merged_coco = {\n",
        "                'images': all_images,\n",
        "                'annotations': all_annotations,\n",
        "                'categories': list(categories_map.values())\n",
        "            }\n",
        "            \n",
        "            merged_ann_file = merged_train / \"_annotations.coco.json\"\n",
        "            with open(merged_ann_file, 'w') as f:\n",
        "                json.dump(merged_coco, f, indent=2)\n",
        "            \n",
        "            ORIGINAL_DATASET = str(merged_dir)\n",
        "            print(f\"   ‚úÖ Merged {len(all_images)} images\")\n",
        "        elif len(valid_folders) == 1:\n",
        "            ORIGINAL_DATASET = str(valid_folders[0])\n",
        "            print(f\"‚úÖ Using single dataset: {ORIGINAL_DATASET}\")\n",
        "        else:\n",
        "            raise ValueError(f\"Expected 3+ datasets or 1 merged dataset. Found {len(valid_folders)} folders.\")\n",
        "    \n",
        "    # Find query_images (for evaluation)\n",
        "    query_folders = [f for f in Path(DATASET_BASE).iterdir() \n",
        "                     if f.is_dir() and 'query' in f.name.lower() \n",
        "                     and f.name not in {'__MACOSX', '.DS_Store'}]\n",
        "    \n",
        "    if query_folders and (query_folders[0] / \"train\" / \"_annotations.coco.json\").exists():\n",
        "        QUERY_DATASET = str(query_folders[0])\n",
        "        print(f\"‚úÖ Found query_images: {QUERY_DATASET}\")\n",
        "    else:\n",
        "        QUERY_DATASET = ORIGINAL_DATASET\n",
        "        print(f\"‚ö†Ô∏è query_images not found, using training dataset for evaluation\")\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"‚úÖ Training dataset: {ORIGINAL_DATASET}\")\n",
        "    print(f\"‚úÖ Query dataset: {QUERY_DATASET}\")\n",
        "    print(f\"{'='*60}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è USE_EXISTING_DATASETS is False. Run Cell 8 to download datasets.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Download Private Dataset from Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gdown\n",
        "from pathlib import Path\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "# Google Drive file IDs\n",
        "PRIVATE_FILE_IDS = [\n",
        "    \"1ClFqB6vvVXqmru4hA5JhkphyfEEwjyte\",  # Original dataset\n",
        "    \"1Y7jh8lTfAuTqIaDkrF9AS8XJaSAfsGwu\",  # Additional dataset 1\n",
        "    \"1GEF0-6MVMrwSGdvbZtcIp9rB82-wqkDL\"    # Additional dataset 2\n",
        "]\n",
        "\n",
        "# Download location\n",
        "DATASET_BASE = \"/content/drive/MyDrive/credit_card_yolov12/datasets\"\n",
        "Path(DATASET_BASE).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Main extract directory\n",
        "extract_dir = f\"{DATASET_BASE}/private_dataset\"\n",
        "Path(extract_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Download and extract all datasets\n",
        "print(f\"Downloading {len(PRIVATE_FILE_IDS)} datasets from Google Drive...\\n\")\n",
        "\n",
        "for idx, file_id in enumerate(PRIVATE_FILE_IDS, 1):\n",
        "    zip_path = f\"{DATASET_BASE}/private_dataset_{idx}.zip\"\n",
        "    print(f\"[{idx}/{len(PRIVATE_FILE_IDS)}] Downloading dataset {idx}...\")\n",
        "    print(f\"   File ID: {file_id}\")\n",
        "    \n",
        "    # Download using gdown\n",
        "    url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "    try:\n",
        "        gdown.download(url, zip_path, quiet=False)\n",
        "        \n",
        "        # Extract dataset\n",
        "        if Path(zip_path).exists():\n",
        "            print(f\"   Extracting to: {extract_dir}\")\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(extract_dir)\n",
        "            print(f\"   ‚úÖ Dataset {idx} extracted successfully\\n\")\n",
        "            \n",
        "            # Clean up zip file\n",
        "            Path(zip_path).unlink()\n",
        "        else:\n",
        "            print(f\"   ‚ö†Ô∏è Warning: Download failed for dataset {idx}\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Error downloading dataset {idx}: {e}\\n\")\n",
        "\n",
        "# Find the actual dataset folder(s)\n",
        "print(f\"\\nLooking for dataset folders in: {extract_dir}\")\n",
        "print(f\"=\" * 60)\n",
        "\n",
        "# Skip macOS metadata folders\n",
        "skip_folders = {'__MACOSX', '.DS_Store'}\n",
        "\n",
        "# Check if there's a train folder directly in extract_dir\n",
        "train_in_root = Path(extract_dir) / \"train\"\n",
        "if train_in_root.exists():\n",
        "    ORIGINAL_DATASET = extract_dir\n",
        "    print(f\"‚úÖ Found dataset structure in root: {ORIGINAL_DATASET}\")\n",
        "else:\n",
        "    # Find all folders, excluding macOS metadata\n",
        "    all_folders = [d for d in Path(extract_dir).iterdir() if d.is_dir() and d.name not in skip_folders]\n",
        "    \n",
        "    if not all_folders:\n",
        "        ORIGINAL_DATASET = extract_dir\n",
        "        print(f\"‚ö†Ô∏è No dataset folders found, using extract directory: {ORIGINAL_DATASET}\")\n",
        "    else:\n",
        "        print(f\"Found {len(all_folders)} folder(s) (excluding macOS metadata):\\n\")\n",
        "        \n",
        "        # Check each folder for dataset structure\n",
        "        dataset_candidates = []\n",
        "        for folder in sorted(all_folders):  # Sort for consistent ordering\n",
        "            # Double-check: skip __MACOSX and other metadata folders\n",
        "            if folder.name in skip_folders or '__MACOSX' in folder.name or '.DS_Store' in folder.name:\n",
        "                print(f\"   ‚ùå SKIP {folder.name} - Metadata folder\")\n",
        "                continue\n",
        "                \n",
        "            train_check = folder / \"train\"\n",
        "            ann_check = list(folder.glob(\"**/_annotations.coco.json\"))\n",
        "            has_images = len(list(folder.glob(\"**/*.jpg\"))) + len(list(folder.glob(\"**/*.png\"))) > 0\n",
        "            image_count = len(list(folder.glob(\"**/*.jpg\"))) + len(list(folder.glob(\"**/*.png\")))\n",
        "            \n",
        "            if train_check.exists():\n",
        "                train_images = len(list((train_check).glob(\"*.jpg\"))) + len(list((train_check).glob(\"*.png\")))\n",
        "                train_anns = len(list((train_check).glob(\"*.json\")))\n",
        "                print(f\"   ‚úÖ {folder.name} - Has 'train' folder ({train_images} images, {train_anns} annotations)\")\n",
        "                dataset_candidates.append((folder, \"train\", True, train_images, folder.name))\n",
        "            elif ann_check:\n",
        "                print(f\"   ‚úÖ {folder.name} - Has COCO annotations ({len(ann_check)} files, {image_count} images)\")\n",
        "                dataset_candidates.append((folder, \"annotations\", True, image_count, folder.name))\n",
        "            elif has_images:\n",
        "                print(f\"   üìÅ {folder.name} - Has images ({image_count} images, no annotations found)\")\n",
        "                dataset_candidates.append((folder, \"images\", False, image_count, folder.name))\n",
        "            else:\n",
        "                print(f\"   üìÅ {folder.name} - Empty or unknown structure\")\n",
        "        \n",
        "        # Handle multiple dataset folders - merge all for training\n",
        "        if dataset_candidates:\n",
        "            # Find all datasets with train structure, excluding __MACOSX\n",
        "            train_candidates = [\n",
        "                c for c in dataset_candidates \n",
        "                if c[1] == \"train\" and '__MACOSX' not in c[4] and '.DS_Store' not in c[4]\n",
        "            ]\n",
        "            \n",
        "            # Find query_images separately (for evaluation), excluding __MACOSX\n",
        "            query_candidates = [\n",
        "                c for c in dataset_candidates \n",
        "                if 'query' in c[4].lower() and c[1] == \"train\" \n",
        "                and '__MACOSX' not in c[4] and '.DS_Store' not in c[4]\n",
        "            ]\n",
        "            if query_candidates:\n",
        "                QUERY_DATASET = str(query_candidates[0][0])\n",
        "                print(f\"\\n‚úÖ Found query_images for evaluation: {QUERY_DATASET} ({query_candidates[0][3]} images)\")\n",
        "            else:\n",
        "                # Fallback: use first train dataset as query\n",
        "                if train_candidates:\n",
        "                    QUERY_DATASET = str(train_candidates[0][0])\n",
        "                    print(f\"\\n‚ö†Ô∏è query_images not found, using first train dataset for evaluation: {QUERY_DATASET}\")\n",
        "                else:\n",
        "                    QUERY_DATASET = None\n",
        "            \n",
        "            # Merge all train folders for training\n",
        "            if len(train_candidates) > 1:\n",
        "                print(f\"\\nüì¶ Found {len(train_candidates)} dataset folders with train structure:\")\n",
        "                total_images = sum(c[3] for c in train_candidates)\n",
        "                for c in train_candidates:\n",
        "                    print(f\"   - {c[0].name}: {c[3]} images\")\n",
        "                \n",
        "                # Create merged dataset directory\n",
        "                merged_dir = Path(extract_dir) / \"merged_dataset\"\n",
        "                merged_dir.mkdir(exist_ok=True)\n",
        "                merged_train = merged_dir / \"train\"\n",
        "                merged_train.mkdir(exist_ok=True)\n",
        "                \n",
        "                print(f\"\\nüîÑ Merging all datasets for training: {merged_dir}\")\n",
        "                \n",
        "                # Merge all train folders\n",
        "                import shutil\n",
        "                import json\n",
        "                from collections import defaultdict\n",
        "                \n",
        "                all_images = []\n",
        "                all_annotations = []\n",
        "                image_id_offset = 0\n",
        "                ann_id_offset = 0\n",
        "                categories_map = {}\n",
        "                \n",
        "                for folder_path, _, _, _ in train_candidates:\n",
        "                    train_folder = folder_path / \"train\"\n",
        "                    ann_file = train_folder / \"_annotations.coco.json\"\n",
        "                    \n",
        "                    if ann_file.exists():\n",
        "                        with open(ann_file, 'r') as f:\n",
        "                            coco_data = json.load(f)\n",
        "                        \n",
        "                        # Merge categories (assuming same categories)\n",
        "                        if not categories_map:\n",
        "                            categories_map = {cat['id']: cat for cat in coco_data.get('categories', [])}\n",
        "                        \n",
        "                        # Copy images and update IDs\n",
        "                        for img in coco_data.get('images', []):\n",
        "                            img['id'] = image_id_offset + img['id']\n",
        "                            src_img = train_folder / img['file_name']\n",
        "                            if src_img.exists():\n",
        "                                dst_img = merged_train / img['file_name']\n",
        "                                # Handle filename conflicts\n",
        "                                if dst_img.exists():\n",
        "                                    stem = Path(img['file_name']).stem\n",
        "                                    ext = Path(img['file_name']).suffix\n",
        "                                    img['file_name'] = f\"{folder_path.name}_{stem}{ext}\"\n",
        "                                    dst_img = merged_train / img['file_name']\n",
        "                                shutil.copy2(src_img, dst_img)\n",
        "                                all_images.append(img)\n",
        "                        \n",
        "                        # Update annotation IDs\n",
        "                        for ann in coco_data.get('annotations', []):\n",
        "                            ann['id'] = ann_id_offset + ann['id']\n",
        "                            ann['image_id'] = image_id_offset + ann['image_id']\n",
        "                            all_annotations.append(ann)\n",
        "                        \n",
        "                        image_id_offset = max(img['id'] for img in all_images) + 1\n",
        "                        ann_id_offset = max(ann['id'] for ann in all_annotations) + 1\n",
        "                \n",
        "                # Create merged COCO annotation file\n",
        "                merged_coco = {\n",
        "                    'images': all_images,\n",
        "                    'annotations': all_annotations,\n",
        "                    'categories': list(categories_map.values())\n",
        "                }\n",
        "                \n",
        "                merged_ann_file = merged_train / \"_annotations.coco.json\"\n",
        "                with open(merged_ann_file, 'w') as f:\n",
        "                    json.dump(merged_coco, f, indent=2)\n",
        "                \n",
        "                print(f\"   ‚úÖ Merged {len(all_images)} images and {len(all_annotations)} annotations\")\n",
        "                print(f\"   ‚úÖ Created merged dataset: {merged_dir}\")\n",
        "                \n",
        "                ORIGINAL_DATASET = str(merged_dir)\n",
        "            else:\n",
        "                # Single train folder\n",
        "                ORIGINAL_DATASET = str(train_candidates[0][0])\n",
        "                print(f\"\\n‚úÖ Using single dataset: {ORIGINAL_DATASET} ({train_candidates[0][3]} images)\")\n",
        "                if not QUERY_DATASET:\n",
        "                    QUERY_DATASET = ORIGINAL_DATASET\n",
        "        else:\n",
        "            ORIGINAL_DATASET = extract_dir\n",
        "            print(f\"\\n‚ö†Ô∏è No valid dataset structure found, using extract directory\")\n",
        "\n",
        "# FINAL SAFETY CHECK: Ensure ORIGINAL_DATASET is never __MACOSX\n",
        "if '__MACOSX' in str(ORIGINAL_DATASET) or Path(ORIGINAL_DATASET).name == '__MACOSX':\n",
        "    print(f\"\\n‚ùå CRITICAL ERROR: ORIGINAL_DATASET is set to __MACOSX!\")\n",
        "    print(f\"   Attempting to fix...\")\n",
        "    \n",
        "    # Search for valid datasets\n",
        "    DATASET_BASE = Path(extract_dir)\n",
        "    valid_folders = []\n",
        "    for folder in DATASET_BASE.iterdir():\n",
        "        if folder.is_dir() and folder.name not in {'__MACOSX', '.DS_Store'}:\n",
        "            train_check = folder / \"train\" / \"_annotations.coco.json\"\n",
        "            if train_check.exists():\n",
        "                valid_folders.append(folder)\n",
        "    \n",
        "    if valid_folders:\n",
        "        # Prefer query_images, then merged_dataset, then any valid\n",
        "        query_folders = [f for f in valid_folders if 'query' in f.name.lower()]\n",
        "        merged_folders = [f for f in valid_folders if 'merged' in f.name.lower()]\n",
        "        \n",
        "        if merged_folders:\n",
        "            ORIGINAL_DATASET = str(merged_folders[0])\n",
        "            print(f\"   ‚úÖ Fixed: Using merged dataset: {ORIGINAL_DATASET}\")\n",
        "        elif query_folders:\n",
        "            ORIGINAL_DATASET = str(query_folders[0])\n",
        "            print(f\"   ‚úÖ Fixed: Using query_images: {ORIGINAL_DATASET}\")\n",
        "        else:\n",
        "            ORIGINAL_DATASET = str(valid_folders[0])\n",
        "            print(f\"   ‚úÖ Fixed: Using first valid dataset: {ORIGINAL_DATASET}\")\n",
        "    else:\n",
        "        raise ValueError(\"No valid dataset folders found! Check Cell 8 output.\")\n",
        "\n",
        "# Verify annotation file exists\n",
        "ann_check = Path(ORIGINAL_DATASET) / \"train\" / \"_annotations.coco.json\"\n",
        "if not ann_check.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Annotation file not found: {ann_check}\\n\"\n",
        "        f\"ORIGINAL_DATASET: {ORIGINAL_DATASET}\\n\"\n",
        "        f\"Please check Cell 8 output.\"\n",
        "    )\n",
        "\n",
        "print(f\"\\n{'=' * 60}\")\n",
        "print(f\"üìã Training Strategy:\")\n",
        "print(f\"   ‚úÖ Training: Merged dataset (query_images + left_rotate + right_rotate)\")\n",
        "print(f\"   ‚úÖ Evaluation: query_images test set only\")\n",
        "print(f\"   ‚úÖ Occlusion levels: 0%, 25%, 75%, 100%\")\n",
        "print(f\"\\n{'=' * 60}\")\n",
        "print(f\"‚úÖ Training dataset: {ORIGINAL_DATASET}\")\n",
        "print(f\"‚úÖ Query dataset (for evaluation): {QUERY_DATASET if 'QUERY_DATASET' in locals() else 'Not set'}\")\n",
        "print(f\"   Training dataset exists: {Path(ORIGINAL_DATASET).exists()}\")\n",
        "print(f\"   Annotation file exists: {ann_check.exists()}\")\n",
        "if 'QUERY_DATASET' in locals() and QUERY_DATASET:\n",
        "    print(f\"   Query dataset exists: {Path(QUERY_DATASET).exists()}\")\n",
        "\n",
        "# Verify the dataset path is accessible and show structure\n",
        "if Path(ORIGINAL_DATASET).exists():\n",
        "    items = list(Path(ORIGINAL_DATASET).iterdir())\n",
        "    print(f\"   Contains {len(items)} items\")\n",
        "    if items:\n",
        "        print(f\"\\n   Folder contents:\")\n",
        "        for item in sorted(items)[:10]:  # Show first 10 items\n",
        "            if item.is_dir():\n",
        "                sub_items = len(list(item.iterdir()))\n",
        "                print(f\"      üìÅ {item.name}/ ({sub_items} items)\")\n",
        "            else:\n",
        "                size_mb = item.stat().st_size / (1024 * 1024) if item.is_file() else 0\n",
        "                print(f\"      üìÑ {item.name} ({size_mb:.2f} MB)\" if size_mb > 0 else f\"      üìÑ {item.name}\")\n",
        "        \n",
        "        # Check for train folder specifically\n",
        "        train_path = Path(ORIGINAL_DATASET) / \"train\"\n",
        "        if train_path.exists():\n",
        "            train_images = len(list(train_path.glob(\"*.jpg\"))) + len(list(train_path.glob(\"*.png\")))\n",
        "            train_anns = len(list(train_path.glob(\"*.json\")))\n",
        "            print(f\"\\n   ‚úÖ Train folder found: {train_images} images, {train_anns} annotation files\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Prepare Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Add src to path\n",
        "sys.path.insert(0, '/content/credit_card_yolov12')\n",
        "\n",
        "# Verify dataset structure\n",
        "print(f\"Dataset location: {ORIGINAL_DATASET}\")\n",
        "print(f\"Dataset exists: {Path(ORIGINAL_DATASET).exists()}\")\n",
        "\n",
        "if Path(ORIGINAL_DATASET).exists():\n",
        "    print(f\"\\nDataset contents:\")\n",
        "    for item in sorted(Path(ORIGINAL_DATASET).iterdir()):\n",
        "        item_type = \"üìÅ\" if item.is_dir() else \"üìÑ\"\n",
        "        print(f\"  {item_type} {item.name}\")\n",
        "    \n",
        "    # Check for train folder\n",
        "    train_path = Path(ORIGINAL_DATASET) / \"train\"\n",
        "    if train_path.exists():\n",
        "        train_files = list(train_path.glob(\"*.json\"))\n",
        "        train_images = list(train_path.glob(\"*.jpg\")) + list(train_path.glob(\"*.png\"))\n",
        "        print(f\"\\n‚úÖ Train folder found!\")\n",
        "        print(f\"   - Annotations: {len(train_files)}\")\n",
        "        print(f\"   - Images: {len(train_images)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split merged dataset for training\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Ensure we're in the repo directory\n",
        "REPO_DIR = \"/content/credit_card_yolov12\"\n",
        "os.chdir(REPO_DIR)\n",
        "sys.path.insert(0, REPO_DIR)\n",
        "\n",
        "# CRITICAL: Verify ORIGINAL_DATASET is valid before splitting\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"VERIFYING DATASET BEFORE SPLITTING\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "# Check if ORIGINAL_DATASET exists and is valid\n",
        "if 'ORIGINAL_DATASET' not in locals():\n",
        "    raise ValueError(\"ORIGINAL_DATASET not set! Run Cell 7 or Cell 8 first.\")\n",
        "\n",
        "print(f\"ORIGINAL_DATASET: {ORIGINAL_DATASET}\")\n",
        "print(f\"Path exists: {Path(ORIGINAL_DATASET).exists()}\")\n",
        "\n",
        "# Check if __MACOSX is in the path\n",
        "if '__MACOSX' in str(ORIGINAL_DATASET):\n",
        "    print(f\"\\n‚ùå ERROR: ORIGINAL_DATASET contains __MACOSX!\")\n",
        "    print(f\"   Searching for valid dataset...\")\n",
        "    \n",
        "    # Find valid dataset\n",
        "    DATASET_BASE = \"/content/drive/MyDrive/credit_card_yolov12/datasets/private_dataset\"\n",
        "    valid_folders = []\n",
        "    \n",
        "    if Path(DATASET_BASE).exists():\n",
        "        for folder in Path(DATASET_BASE).iterdir():\n",
        "            if folder.is_dir() and folder.name not in {'__MACOSX', '.DS_Store'}:\n",
        "                train_check = folder / \"train\" / \"_annotations.coco.json\"\n",
        "                if train_check.exists():\n",
        "                    valid_folders.append(folder)\n",
        "                    print(f\"   ‚úÖ Found valid dataset: {folder.name}\")\n",
        "    \n",
        "    if valid_folders:\n",
        "        # Prefer merged_dataset, then query_images, then any valid\n",
        "        merged_folders = [f for f in valid_folders if 'merged' in f.name.lower()]\n",
        "        query_folders = [f for f in valid_folders if 'query' in f.name.lower()]\n",
        "        \n",
        "        if merged_folders:\n",
        "            ORIGINAL_DATASET = str(merged_folders[0])\n",
        "            print(f\"   ‚úÖ Fixed: Using merged dataset: {ORIGINAL_DATASET}\")\n",
        "        elif query_folders:\n",
        "            ORIGINAL_DATASET = str(query_folders[0])\n",
        "            print(f\"   ‚úÖ Fixed: Using query_images: {ORIGINAL_DATASET}\")\n",
        "        else:\n",
        "            ORIGINAL_DATASET = str(valid_folders[0])\n",
        "            print(f\"   ‚úÖ Fixed: Using first valid dataset: {ORIGINAL_DATASET}\")\n",
        "    else:\n",
        "        raise ValueError(\"No valid dataset folders found! Please check your Drive folder.\")\n",
        "\n",
        "# Verify annotation file exists\n",
        "ann_file = Path(ORIGINAL_DATASET) / \"train\" / \"_annotations.coco.json\"\n",
        "if not ann_file.exists():\n",
        "    print(f\"\\n‚ùå Annotation file not found: {ann_file}\")\n",
        "    print(f\"   ORIGINAL_DATASET: {ORIGINAL_DATASET}\")\n",
        "    print(f\"   Path exists: {Path(ORIGINAL_DATASET).exists()}\")\n",
        "    \n",
        "    if Path(ORIGINAL_DATASET).exists():\n",
        "        print(f\"   Contents of ORIGINAL_DATASET:\")\n",
        "        for item in sorted(Path(ORIGINAL_DATASET).iterdir()):\n",
        "            print(f\"      - {item.name} ({'dir' if item.is_dir() else 'file'})\")\n",
        "    \n",
        "    # Try to find annotation in alternative locations\n",
        "    alt_locations = [\n",
        "        Path(ORIGINAL_DATASET) / \"_annotations.coco.json\",\n",
        "        Path(ORIGINAL_DATASET).parent / \"train\" / \"_annotations.coco.json\",\n",
        "    ]\n",
        "    \n",
        "    found_alt = False\n",
        "    for alt_loc in alt_locations:\n",
        "        if alt_loc.exists():\n",
        "            print(f\"   ‚ö†Ô∏è Found annotation at alternative location: {alt_loc}\")\n",
        "            # Copy to expected location\n",
        "            train_dir = Path(ORIGINAL_DATASET) / \"train\"\n",
        "            train_dir.mkdir(exist_ok=True)\n",
        "            import shutil\n",
        "            shutil.copy2(alt_loc, ann_file)\n",
        "            print(f\"   ‚úÖ Copied annotation to: {ann_file}\")\n",
        "            found_alt = True\n",
        "            break\n",
        "    \n",
        "    if not found_alt:\n",
        "        raise FileNotFoundError(\n",
        "            f\"Annotation file not found: {ann_file}\\n\"\n",
        "            f\"Please check Cell 7/8 output and ensure a valid dataset was selected.\"\n",
        "        )\n",
        "\n",
        "print(f\"‚úÖ Annotation file found: {ann_file}\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "# Verify script exists\n",
        "script_path = Path(REPO_DIR) / \"src\" / \"split_dataset.py\"\n",
        "if script_path.exists():\n",
        "    print(f\"‚úÖ Found script: {script_path}\")\n",
        "    print(f\"üìã Splitting dataset for training: {ORIGINAL_DATASET}\")\n",
        "    !python src/split_dataset.py --dataset {ORIGINAL_DATASET} --seed 42\n",
        "else:\n",
        "    print(f\"‚ùå Script not found at: {script_path}\")\n",
        "\n",
        "# Also split query_images separately for evaluation\n",
        "if 'QUERY_DATASET' in locals() and QUERY_DATASET and QUERY_DATASET != ORIGINAL_DATASET:\n",
        "    # Verify query dataset too\n",
        "    query_ann = Path(QUERY_DATASET) / \"train\" / \"_annotations.coco.json\"\n",
        "    if query_ann.exists():\n",
        "        print(f\"\\nüìã Splitting query_images for evaluation: {QUERY_DATASET}\")\n",
        "        !python src/split_dataset.py --dataset {QUERY_DATASET} --seed 42\n",
        "    else:\n",
        "        print(f\"\\n‚ö†Ô∏è Query dataset annotation not found: {query_ann}\")\n",
        "        print(f\"   Skipping query dataset split\")\n",
        "else:\n",
        "    print(f\"\\n‚úÖ Query dataset same as training dataset, already split\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate progressive occlusion test sets from query_images test set only\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Ensure we're in the repo directory\n",
        "REPO_DIR = \"/content/credit_card_yolov12\"\n",
        "os.chdir(REPO_DIR)\n",
        "sys.path.insert(0, REPO_DIR)\n",
        "\n",
        "# Find query_images split directory (for evaluation)\n",
        "DATASET_BASE = \"/content/drive/MyDrive/credit_card_yolov12/datasets\"\n",
        "if 'QUERY_DATASET' in locals() and QUERY_DATASET:\n",
        "    # Find the split directory for query_images\n",
        "    query_name = Path(QUERY_DATASET).name\n",
        "    query_split_dirs = list(Path(DATASET_BASE).glob(f\"{query_name}_split\"))\n",
        "    if query_split_dirs:\n",
        "        TEST_DATASET = str(query_split_dirs[0] / \"test\")\n",
        "        print(f\"‚úÖ Found query_images test dataset: {TEST_DATASET}\")\n",
        "    else:\n",
        "        # Fallback: use first split directory\n",
        "        split_dirs = list(Path(DATASET_BASE).glob(\"*_split\"))\n",
        "        if split_dirs:\n",
        "            TEST_DATASET = str(split_dirs[0] / \"test\")\n",
        "            print(f\"‚ö†Ô∏è Using first split test dataset: {TEST_DATASET}\")\n",
        "        else:\n",
        "            raise FileNotFoundError(\"Split dataset not found. Run split_dataset.py first.\")\n",
        "else:\n",
        "    # Fallback: use first split directory\n",
        "    split_dirs = list(Path(DATASET_BASE).glob(\"*_split\"))\n",
        "    if split_dirs:\n",
        "        TEST_DATASET = str(split_dirs[0] / \"test\")\n",
        "        print(f\"‚ö†Ô∏è QUERY_DATASET not set, using first split: {TEST_DATASET}\")\n",
        "    else:\n",
        "        raise FileNotFoundError(\"Split dataset not found. Run split_dataset.py first.\")\n",
        "\n",
        "# Verify script exists\n",
        "script_path = Path(REPO_DIR) / \"src\" / \"prepare_progressive_tests.py\"\n",
        "if script_path.exists():\n",
        "    print(f\"‚úÖ Found script: {script_path}\")\n",
        "    print(f\"üìã Generating occlusion levels: 0%, 25%, 75%, 100%\")\n",
        "    !python src/prepare_progressive_tests.py --test-dataset {TEST_DATASET} --type crop --seed 42 --levels 25 75 100\n",
        "else:\n",
        "    print(f\"‚ùå Script not found at: {script_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Train Model with Pretrained Weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "import yaml\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Model configuration\n",
        "MODEL_SIZE = \"m\"  # n, s, m, l, x\n",
        "EPOCHS = 100\n",
        "IMG_SIZE = 640\n",
        "BATCH = 16\n",
        "\n",
        "# Pretrained weights path\n",
        "PRETRAINED_WEIGHTS = \"/content/drive/MyDrive/yolov12_runs/nonprivate/checkpoints/best.pt\"\n",
        "\n",
        "# Find the actual split dataset directory\n",
        "DATASET_BASE = \"/content/drive/MyDrive/credit_card_yolov12/datasets\"\n",
        "split_dirs = list(Path(DATASET_BASE).glob(\"*_split\"))\n",
        "if split_dirs:\n",
        "    split_base = split_dirs[0]\n",
        "    print(f\"‚úÖ Found split dataset: {split_base.name}\")\n",
        "    TRAIN_DATASET = str(split_base / \"train\")\n",
        "    VAL_DATASET = str(split_base / \"val\")\n",
        "else:\n",
        "    raise FileNotFoundError(\"Split dataset not found\")\n",
        "\n",
        "print(f\"Training on: {TRAIN_DATASET}\")\n",
        "print(f\"Validating on: {VAL_DATASET}\")\n",
        "print(f\"Pretrained weights: {PRETRAINED_WEIGHTS}\")\n",
        "print(f\"Pretrained weights exists: {Path(PRETRAINED_WEIGHTS).exists()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare dataset config for YOLOv8\n",
        "def prepare_yolo_config(train_path, val_path, output_config=\"dataset.yaml\"):\n",
        "    \"\"\"Create YOLOv8 dataset config.\"\"\"\n",
        "    # Find annotation file (could be in train_path or train_path/train/)\n",
        "    train_path_obj = Path(train_path)\n",
        "    ann_file = train_path_obj / \"_annotations.coco.json\"\n",
        "    \n",
        "    if not ann_file.exists():\n",
        "        # Try nested structure\n",
        "        ann_file = train_path_obj / \"train\" / \"_annotations.coco.json\"\n",
        "    \n",
        "    if not ann_file.exists():\n",
        "        raise FileNotFoundError(\n",
        "            f\"Annotation file not found. Checked:\\n\"\n",
        "            f\"  - {train_path_obj / '_annotations.coco.json'}\\n\"\n",
        "            f\"  - {train_path_obj / 'train' / '_annotations.coco.json'}\"\n",
        "        )\n",
        "    \n",
        "    print(f\"Loading annotations from: {ann_file}\")\n",
        "    with open(ann_file, 'r') as f:\n",
        "        coco_data = json.load(f)\n",
        "    \n",
        "    categories = coco_data.get('categories', [])\n",
        "    class_names = [cat['name'] for cat in sorted(categories, key=lambda x: x['id'])]\n",
        "    \n",
        "    # Determine the base path and relative paths\n",
        "    base_path = train_path_obj.parent\n",
        "    \n",
        "    # Create YOLOv8 config\n",
        "    config = {\n",
        "        'path': str(base_path.absolute()),\n",
        "        'train': 'train',\n",
        "        'val': 'val',\n",
        "        'names': {i: name for i, name in enumerate(class_names)},\n",
        "        'nc': len(class_names)\n",
        "    }\n",
        "    \n",
        "    with open(output_config, 'w') as f:\n",
        "        yaml.dump(config, f, default_flow_style=False)\n",
        "    \n",
        "    print(f\"Dataset config created: {output_config}\")\n",
        "    print(f\"Base path: {base_path}\")\n",
        "    print(f\"Classes ({len(class_names)}): {class_names}\")\n",
        "    return output_config\n",
        "\n",
        "config_file = prepare_yolo_config(TRAIN_DATASET, VAL_DATASET)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert COCO to YOLO format\n",
        "from src.train import convert_coco_to_yolo\n",
        "\n",
        "# Convert train set\n",
        "convert_coco_to_yolo(TRAIN_DATASET)\n",
        "\n",
        "# Convert val set  \n",
        "convert_coco_to_yolo(VAL_DATASET)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize model with pretrained weights\n",
        "if Path(PRETRAINED_WEIGHTS).exists():\n",
        "    print(f\"Loading pretrained weights from: {PRETRAINED_WEIGHTS}\")\n",
        "    model = YOLO(PRETRAINED_WEIGHTS)\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è Pretrained weights not found, using default YOLOv8{MODEL_SIZE}\")\n",
        "    model = YOLO(f\"yolov8{MODEL_SIZE}.pt\")\n",
        "\n",
        "# Train model\n",
        "# WandB is already initialized in Step 2, so it will automatically log training metrics\n",
        "results = model.train(\n",
        "    data=config_file,\n",
        "    epochs=EPOCHS,\n",
        "    imgsz=IMG_SIZE,\n",
        "    batch=BATCH,\n",
        "    project=\"/content/drive/MyDrive/credit_card_yolov12/models\",\n",
        "    name=f\"private_objects_{MODEL_SIZE}\",\n",
        "    exist_ok=True,\n",
        "    save=True,\n",
        "    plots=True,\n",
        "    val=True,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Save Model to Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# Paths\n",
        "MODEL_DIR = results.save_dir\n",
        "DRIVE_MODEL_DIR = f\"/content/drive/MyDrive/credit_card_yolov12/models/private_objects_{MODEL_SIZE}\"\n",
        "\n",
        "# Check if models are already in Drive (they are, since training saves directly to Drive)\n",
        "if str(MODEL_DIR) == DRIVE_MODEL_DIR:\n",
        "    print(f\"‚úÖ Models already saved to Drive: {MODEL_DIR}\")\n",
        "    print(f\"   Best model: {Path(MODEL_DIR) / 'weights' / 'best.pt'}\")\n",
        "    print(f\"   Last model: {Path(MODEL_DIR) / 'weights' / 'last.pt'}\")\n",
        "    DRIVE_MODEL_DIR = MODEL_DIR  # Use the same directory\n",
        "else:\n",
        "    # Copy best model to Drive\n",
        "    best_model = Path(MODEL_DIR) / \"weights\" / \"best.pt\"\n",
        "    last_model = Path(MODEL_DIR) / \"weights\" / \"last.pt\"\n",
        "    \n",
        "    Path(DRIVE_MODEL_DIR).mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    if best_model.exists():\n",
        "        shutil.copy2(best_model, f\"{DRIVE_MODEL_DIR}/best.pt\")\n",
        "        print(f\"‚úÖ Best model saved to: {DRIVE_MODEL_DIR}/best.pt\")\n",
        "    \n",
        "    if last_model.exists():\n",
        "        shutil.copy2(last_model, f\"{DRIVE_MODEL_DIR}/last.pt\")\n",
        "        print(f\"‚úÖ Last model saved to: {DRIVE_MODEL_DIR}/last.pt\")\n",
        "\n",
        "# Set DRIVE_MODEL_DIR for use in next cells\n",
        "print(f\"\\n‚úÖ Model directory: {DRIVE_MODEL_DIR}\")\n",
        "print(f\"‚úÖ Best model path: {Path(DRIVE_MODEL_DIR) / 'weights' / 'best.pt'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Evaluate on Progressive Occlusion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on progressive occlusion test sets\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Ensure we're in the repo directory\n",
        "REPO_DIR = \"/content/credit_card_yolov12\"\n",
        "os.chdir(REPO_DIR)\n",
        "sys.path.insert(0, REPO_DIR)\n",
        "\n",
        "# Find the actual split directory\n",
        "DATASET_BASE = \"/content/drive/MyDrive/credit_card_yolov12/datasets\"\n",
        "split_dirs = list(Path(DATASET_BASE).glob(\"*_split\"))\n",
        "if split_dirs:\n",
        "    TEST_SETS_BASE = str(split_dirs[0])\n",
        "    print(f\"‚úÖ Found split directory: {TEST_SETS_BASE}\")\n",
        "else:\n",
        "    TEST_SETS_BASE = f\"{DATASET_BASE}/private_dataset_split\"\n",
        "    print(f\"‚ö†Ô∏è Using fallback: {TEST_SETS_BASE}\")\n",
        "\n",
        "# Verify test occlusion directories exist\n",
        "print(f\"\\nChecking test occlusion directories:\")\n",
        "for level in [0, 25, 50, 75]:\n",
        "    test_dir = Path(TEST_SETS_BASE) / f\"test_occlusion_{level}\"\n",
        "    if test_dir.exists():\n",
        "        ann_file = test_dir / \"train\" / \"_annotations.coco.json\"\n",
        "        if ann_file.exists():\n",
        "            print(f\"  ‚úÖ test_occlusion_{level}: {ann_file}\")\n",
        "        else:\n",
        "            print(f\"  ‚ö†Ô∏è test_occlusion_{level}: exists but no annotation file\")\n",
        "    else:\n",
        "        print(f\"  ‚ùå test_occlusion_{level}: not found\")\n",
        "\n",
        "BEST_MODEL = f\"{DRIVE_MODEL_DIR}/weights/best.pt\"\n",
        "print(f\"\\nBest model: {BEST_MODEL}\")\n",
        "print(f\"Test sets base: {TEST_SETS_BASE}\")\n",
        "print(f\"Best model exists: {Path(BEST_MODEL).exists()}\")\n",
        "\n",
        "# Verify script exists\n",
        "script_path = Path(REPO_DIR) / \"src\" / \"evaluate_progressive.py\"\n",
        "if script_path.exists():\n",
        "    print(f\"‚úÖ Found script: {script_path}\")\n",
        "    !python src/evaluate_progressive.py \\\n",
        "        --model {BEST_MODEL} \\\n",
        "        --test-sets {TEST_SETS_BASE} \\\n",
        "        --output /content/drive/MyDrive/credit_card_yolov12/outputs/progressive_evaluation_private\n",
        "else:\n",
        "    print(f\"‚ùå Script not found at: {script_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Log Results to WandB\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Load progressive evaluation results\n",
        "results_file = \"/content/drive/MyDrive/credit_card_yolov12/outputs/progressive_evaluation_private/progressive_results.json\"\n",
        "\n",
        "if Path(results_file).exists():\n",
        "    with open(results_file, 'r') as f:\n",
        "        eval_results = json.load(f)\n",
        "    \n",
        "    # Log to wandb\n",
        "    for occlusion_level, metrics in eval_results.items():\n",
        "        wandb.log({\n",
        "            f\"mAP50_occlusion_{occlusion_level}\": metrics['mAP50'],\n",
        "            f\"mAP50_95_occlusion_{occlusion_level}\": metrics['mAP50_95'],\n",
        "            f\"precision_occlusion_{occlusion_level}\": metrics['precision'],\n",
        "            f\"recall_occlusion_{occlusion_level}\": metrics['recall'],\n",
        "            f\"f1_occlusion_{occlusion_level}\": metrics['f1']\n",
        "        })\n",
        "    \n",
        "    # Log visualization\n",
        "    plot_file = \"/content/drive/MyDrive/credit_card_yolov12/outputs/progressive_evaluation_private/progressive_occlusion_results.png\"\n",
        "    if Path(plot_file).exists():\n",
        "        wandb.log({\"progressive_occlusion_plot\": wandb.Image(plot_file)})\n",
        "    \n",
        "    print(\"‚úÖ Results logged to Weights & Biases\")\n",
        "    print(\"\\nProgressive Occlusion Results:\")\n",
        "    df = pd.DataFrame(eval_results).T\n",
        "    print(df)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Results file not found\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Finish wandb run\n",
        "wandb.finish()\n",
        "print(\"‚úÖ Training complete! Check your Weights & Biases dashboard.\")\n",
        "print(f\"‚úÖ Model saved to: {DRIVE_MODEL_DIR}\")\n",
        "print(f\"‚úÖ Results saved to: /content/drive/MyDrive/credit_card_yolov12/outputs/progressive_evaluation_private\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
