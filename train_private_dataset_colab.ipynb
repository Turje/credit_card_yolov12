{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Private Dataset Training - YOLOv8\n",
        "\n",
        "This notebook trains a YOLOv8 model for private object detection with 16 categories.\n",
        "\n",
        "Features:\n",
        "- Uses pretrained weights from non-private dataset\n",
        "- 16 categories\n",
        "- Progressive occlusion evaluation (crop type)\n",
        "- Weights & Biases integration\n",
        "- Google Drive integration\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Setup Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "%pip install -q ultralytics roboflow python-dotenv opencv-python matplotlib numpy pandas pyyaml wandb gdown\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone or update your repository\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "REPO_DIR = \"/content/credit_card_yolov12\"\n",
        "\n",
        "if Path(REPO_DIR).exists():\n",
        "    print(\"Repository exists, pulling latest changes...\")\n",
        "    %cd {REPO_DIR}\n",
        "    !git pull origin main\n",
        "else:\n",
        "    print(\"Cloning repository...\")\n",
        "    !git clone https://github.com/Turje/credit_card_yolov12.git\n",
        "    %cd credit_card_yolov12\n",
        "\n",
        "# Verify files exist\n",
        "print(\"\\nVerifying required files...\")\n",
        "required_files = [\n",
        "    \"src/split_dataset.py\",\n",
        "    \"src/prepare_progressive_tests.py\",\n",
        "    \"src/train.py\",\n",
        "    \"src/evaluate_progressive.py\"\n",
        "]\n",
        "\n",
        "for file in required_files:\n",
        "    file_path = Path(REPO_DIR) / file\n",
        "    if file_path.exists():\n",
        "        print(f\"‚úÖ {file}\")\n",
        "    else:\n",
        "        print(f\"‚ùå {file} - NOT FOUND!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Setup Weights & Biases\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "# Login to Weights & Biases\n",
        "wandb.login(key='6defa0781045a6f791ddd5b18bd7ebbdcdfdc86d')\n",
        "\n",
        "# Initialize wandb project\n",
        "wandb.init(\n",
        "    project=\"private-object-detection\",\n",
        "    name=\"yolov8-16categories-pretrained\",\n",
        "    config={\n",
        "        \"model_size\": \"m\",\n",
        "        \"epochs\": 100,\n",
        "        \"imgsz\": 640,\n",
        "        \"batch\": 16,\n",
        "        \"num_classes\": 16,\n",
        "        \"pretrained_weights\": \"/content/drive/MyDrive/yolov12_runs/nonprivate/checkpoints/best.pt\",\n",
        "        \"occlusion_type\": \"crop\"\n",
        "    }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Download Private Dataset from Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gdown\n",
        "from pathlib import Path\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "# Google Drive file IDs\n",
        "PRIVATE_FILE_IDS = [\n",
        "    \"1ClFqB6vvVXqmru4hA5JhkphyfEEwjyte\",  # Original dataset\n",
        "    \"1Y7jh8lTfAuTqIaDkrF9AS8XJaSAfsGwu\",  # Additional dataset 1\n",
        "    \"1GEF0-6MVMrwSGdvbZtcIp9rB82-wqkDL\"    # Additional dataset 2\n",
        "]\n",
        "\n",
        "# Download location\n",
        "DATASET_BASE = \"/content/drive/MyDrive/credit_card_yolov12/datasets\"\n",
        "Path(DATASET_BASE).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Main extract directory\n",
        "extract_dir = f\"{DATASET_BASE}/private_dataset\"\n",
        "Path(extract_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Download and extract all datasets\n",
        "print(f\"Downloading {len(PRIVATE_FILE_IDS)} datasets from Google Drive...\\n\")\n",
        "\n",
        "for idx, file_id in enumerate(PRIVATE_FILE_IDS, 1):\n",
        "    zip_path = f\"{DATASET_BASE}/private_dataset_{idx}.zip\"\n",
        "    print(f\"[{idx}/{len(PRIVATE_FILE_IDS)}] Downloading dataset {idx}...\")\n",
        "    print(f\"   File ID: {file_id}\")\n",
        "    \n",
        "    # Download using gdown\n",
        "    url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "    try:\n",
        "        gdown.download(url, zip_path, quiet=False)\n",
        "        \n",
        "        # Extract dataset\n",
        "        if Path(zip_path).exists():\n",
        "            print(f\"   Extracting to: {extract_dir}\")\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(extract_dir)\n",
        "            print(f\"   ‚úÖ Dataset {idx} extracted successfully\\n\")\n",
        "            \n",
        "            # Clean up zip file\n",
        "            Path(zip_path).unlink()\n",
        "        else:\n",
        "            print(f\"   ‚ö†Ô∏è Warning: Download failed for dataset {idx}\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Error downloading dataset {idx}: {e}\\n\")\n",
        "\n",
        "# Find the actual dataset folder(s)\n",
        "print(f\"\\nLooking for dataset folders in: {extract_dir}\")\n",
        "print(f\"=\" * 60)\n",
        "\n",
        "# Skip macOS metadata folders\n",
        "skip_folders = {'__MACOSX', '.DS_Store'}\n",
        "\n",
        "# Check if there's a train folder directly in extract_dir\n",
        "train_in_root = Path(extract_dir) / \"train\"\n",
        "if train_in_root.exists():\n",
        "    ORIGINAL_DATASET = extract_dir\n",
        "    print(f\"‚úÖ Found dataset structure in root: {ORIGINAL_DATASET}\")\n",
        "else:\n",
        "    # Find all folders, excluding macOS metadata\n",
        "    all_folders = [d for d in Path(extract_dir).iterdir() if d.is_dir() and d.name not in skip_folders]\n",
        "    \n",
        "    if not all_folders:\n",
        "        ORIGINAL_DATASET = extract_dir\n",
        "        print(f\"‚ö†Ô∏è No dataset folders found, using extract directory: {ORIGINAL_DATASET}\")\n",
        "    else:\n",
        "        print(f\"Found {len(all_folders)} folder(s) (excluding macOS metadata):\\n\")\n",
        "        \n",
        "        # Check each folder for dataset structure\n",
        "        dataset_candidates = []\n",
        "        for folder in sorted(all_folders):  # Sort for consistent ordering\n",
        "            train_check = folder / \"train\"\n",
        "            ann_check = list(folder.glob(\"**/_annotations.coco.json\"))\n",
        "            has_images = len(list(folder.glob(\"**/*.jpg\"))) + len(list(folder.glob(\"**/*.png\"))) > 0\n",
        "            image_count = len(list(folder.glob(\"**/*.jpg\"))) + len(list(folder.glob(\"**/*.png\")))\n",
        "            \n",
        "            if train_check.exists():\n",
        "                train_images = len(list((train_check).glob(\"*.jpg\"))) + len(list((train_check).glob(\"*.png\")))\n",
        "                train_anns = len(list((train_check).glob(\"*.json\")))\n",
        "                print(f\"   ‚úÖ {folder.name} - Has 'train' folder ({train_images} images, {train_anns} annotations)\")\n",
        "                dataset_candidates.append((folder, \"train\", True, train_images))\n",
        "            elif ann_check:\n",
        "                print(f\"   ‚úÖ {folder.name} - Has COCO annotations ({len(ann_check)} files, {image_count} images)\")\n",
        "                dataset_candidates.append((folder, \"annotations\", True, image_count))\n",
        "            elif has_images:\n",
        "                print(f\"   üìÅ {folder.name} - Has images ({image_count} images, no annotations found)\")\n",
        "                dataset_candidates.append((folder, \"images\", False, image_count))\n",
        "            else:\n",
        "                print(f\"   üìÅ {folder.name} - Empty or unknown structure\")\n",
        "        \n",
        "        # Select the best candidate\n",
        "        if dataset_candidates:\n",
        "            # Prefer folders with train structure\n",
        "            train_candidates = [c for c in dataset_candidates if c[1] == \"train\"]\n",
        "            if train_candidates:\n",
        "                # If multiple train folders, prefer the one with most images\n",
        "                best_train = max(train_candidates, key=lambda x: x[3])\n",
        "                ORIGINAL_DATASET = str(best_train[0])\n",
        "                print(f\"\\n‚úÖ Selected dataset: {ORIGINAL_DATASET} (has train folder with {best_train[3]} images)\")\n",
        "            else:\n",
        "                # Fall back to annotation-based\n",
        "                ann_candidates = [c for c in dataset_candidates if c[1] == \"annotations\"]\n",
        "                if ann_candidates:\n",
        "                    # Prefer the one with most images\n",
        "                    best_ann = max(ann_candidates, key=lambda x: x[3])\n",
        "                    ORIGINAL_DATASET = str(best_ann[0])\n",
        "                    print(f\"\\n‚úÖ Selected dataset: {ORIGINAL_DATASET} (has annotations, {best_ann[3]} images)\")\n",
        "                else:\n",
        "                    # Use folder with most images\n",
        "                    best_images = max(dataset_candidates, key=lambda x: x[3])\n",
        "                    ORIGINAL_DATASET = str(best_images[0])\n",
        "                    print(f\"\\n‚ö†Ô∏è Selected dataset: {ORIGINAL_DATASET} (has {best_images[3]} images, no annotations)\")\n",
        "                    print(f\"   Note: May need manual verification of structure\")\n",
        "        else:\n",
        "            ORIGINAL_DATASET = extract_dir\n",
        "            print(f\"\\n‚ö†Ô∏è No valid dataset structure found, using extract directory\")\n",
        "\n",
        "print(f\"\\n{'=' * 60}\")\n",
        "print(f\"‚úÖ Final dataset path: {ORIGINAL_DATASET}\")\n",
        "print(f\"   Path exists: {Path(ORIGINAL_DATASET).exists()}\")\n",
        "\n",
        "# Verify the dataset path is accessible and show structure\n",
        "if Path(ORIGINAL_DATASET).exists():\n",
        "    items = list(Path(ORIGINAL_DATASET).iterdir())\n",
        "    print(f\"   Contains {len(items)} items\")\n",
        "    if items:\n",
        "        print(f\"\\n   Folder contents:\")\n",
        "        for item in sorted(items)[:10]:  # Show first 10 items\n",
        "            if item.is_dir():\n",
        "                sub_items = len(list(item.iterdir()))\n",
        "                print(f\"      üìÅ {item.name}/ ({sub_items} items)\")\n",
        "            else:\n",
        "                size_mb = item.stat().st_size / (1024 * 1024) if item.is_file() else 0\n",
        "                print(f\"      üìÑ {item.name} ({size_mb:.2f} MB)\" if size_mb > 0 else f\"      üìÑ {item.name}\")\n",
        "        \n",
        "        # Check for train folder specifically\n",
        "        train_path = Path(ORIGINAL_DATASET) / \"train\"\n",
        "        if train_path.exists():\n",
        "            train_images = len(list(train_path.glob(\"*.jpg\"))) + len(list(train_path.glob(\"*.png\")))\n",
        "            train_anns = len(list(train_path.glob(\"*.json\")))\n",
        "            print(f\"\\n   ‚úÖ Train folder found: {train_images} images, {train_anns} annotation files\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Prepare Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Add src to path\n",
        "sys.path.insert(0, '/content/credit_card_yolov12')\n",
        "\n",
        "# Verify dataset structure\n",
        "print(f\"Dataset location: {ORIGINAL_DATASET}\")\n",
        "print(f\"Dataset exists: {Path(ORIGINAL_DATASET).exists()}\")\n",
        "\n",
        "if Path(ORIGINAL_DATASET).exists():\n",
        "    print(f\"\\nDataset contents:\")\n",
        "    for item in sorted(Path(ORIGINAL_DATASET).iterdir()):\n",
        "        item_type = \"üìÅ\" if item.is_dir() else \"üìÑ\"\n",
        "        print(f\"  {item_type} {item.name}\")\n",
        "    \n",
        "    # Check for train folder\n",
        "    train_path = Path(ORIGINAL_DATASET) / \"train\"\n",
        "    if train_path.exists():\n",
        "        train_files = list(train_path.glob(\"*.json\"))\n",
        "        train_images = list(train_path.glob(\"*.jpg\")) + list(train_path.glob(\"*.png\"))\n",
        "        print(f\"\\n‚úÖ Train folder found!\")\n",
        "        print(f\"   - Annotations: {len(train_files)}\")\n",
        "        print(f\"   - Images: {len(train_images)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split dataset\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Ensure we're in the repo directory\n",
        "REPO_DIR = \"/content/credit_card_yolov12\"\n",
        "os.chdir(REPO_DIR)\n",
        "sys.path.insert(0, REPO_DIR)\n",
        "\n",
        "# Verify script exists\n",
        "script_path = Path(REPO_DIR) / \"src\" / \"split_dataset.py\"\n",
        "if script_path.exists():\n",
        "    print(f\"‚úÖ Found script: {script_path}\")\n",
        "    !python src/split_dataset.py --dataset {ORIGINAL_DATASET} --seed 42\n",
        "else:\n",
        "    print(f\"‚ùå Script not found at: {script_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate progressive occlusion test sets (crop type)\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Ensure we're in the repo directory\n",
        "REPO_DIR = \"/content/credit_card_yolov12\"\n",
        "os.chdir(REPO_DIR)\n",
        "sys.path.insert(0, REPO_DIR)\n",
        "\n",
        "# Find the split directory\n",
        "DATASET_BASE = \"/content/drive/MyDrive/credit_card_yolov12/datasets\"\n",
        "split_dirs = list(Path(DATASET_BASE).glob(\"*_split\"))\n",
        "if split_dirs:\n",
        "    TEST_DATASET = str(split_dirs[0] / \"test\")\n",
        "    print(f\"‚úÖ Found test dataset: {TEST_DATASET}\")\n",
        "else:\n",
        "    raise FileNotFoundError(\"Split dataset not found. Run split_dataset.py first.\")\n",
        "\n",
        "# Verify script exists\n",
        "script_path = Path(REPO_DIR) / \"src\" / \"prepare_progressive_tests.py\"\n",
        "if script_path.exists():\n",
        "    print(f\"‚úÖ Found script: {script_path}\")\n",
        "    !python src/prepare_progressive_tests.py --test-dataset {TEST_DATASET} --type crop --seed 42\n",
        "else:\n",
        "    print(f\"‚ùå Script not found at: {script_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Train Model with Pretrained Weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "import yaml\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Model configuration\n",
        "MODEL_SIZE = \"m\"  # n, s, m, l, x\n",
        "EPOCHS = 100\n",
        "IMG_SIZE = 640\n",
        "BATCH = 16\n",
        "\n",
        "# Pretrained weights path\n",
        "PRETRAINED_WEIGHTS = \"/content/drive/MyDrive/yolov12_runs/nonprivate/checkpoints/best.pt\"\n",
        "\n",
        "# Find the actual split dataset directory\n",
        "DATASET_BASE = \"/content/drive/MyDrive/credit_card_yolov12/datasets\"\n",
        "split_dirs = list(Path(DATASET_BASE).glob(\"*_split\"))\n",
        "if split_dirs:\n",
        "    split_base = split_dirs[0]\n",
        "    print(f\"‚úÖ Found split dataset: {split_base.name}\")\n",
        "    TRAIN_DATASET = str(split_base / \"train\")\n",
        "    VAL_DATASET = str(split_base / \"val\")\n",
        "else:\n",
        "    raise FileNotFoundError(\"Split dataset not found\")\n",
        "\n",
        "print(f\"Training on: {TRAIN_DATASET}\")\n",
        "print(f\"Validating on: {VAL_DATASET}\")\n",
        "print(f\"Pretrained weights: {PRETRAINED_WEIGHTS}\")\n",
        "print(f\"Pretrained weights exists: {Path(PRETRAINED_WEIGHTS).exists()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare dataset config for YOLOv8\n",
        "def prepare_yolo_config(train_path, val_path, output_config=\"dataset.yaml\"):\n",
        "    \"\"\"Create YOLOv8 dataset config.\"\"\"\n",
        "    # Find annotation file (could be in train_path or train_path/train/)\n",
        "    train_path_obj = Path(train_path)\n",
        "    ann_file = train_path_obj / \"_annotations.coco.json\"\n",
        "    \n",
        "    if not ann_file.exists():\n",
        "        # Try nested structure\n",
        "        ann_file = train_path_obj / \"train\" / \"_annotations.coco.json\"\n",
        "    \n",
        "    if not ann_file.exists():\n",
        "        raise FileNotFoundError(\n",
        "            f\"Annotation file not found. Checked:\\n\"\n",
        "            f\"  - {train_path_obj / '_annotations.coco.json'}\\n\"\n",
        "            f\"  - {train_path_obj / 'train' / '_annotations.coco.json'}\"\n",
        "        )\n",
        "    \n",
        "    print(f\"Loading annotations from: {ann_file}\")\n",
        "    with open(ann_file, 'r') as f:\n",
        "        coco_data = json.load(f)\n",
        "    \n",
        "    categories = coco_data.get('categories', [])\n",
        "    class_names = [cat['name'] for cat in sorted(categories, key=lambda x: x['id'])]\n",
        "    \n",
        "    # Determine the base path and relative paths\n",
        "    base_path = train_path_obj.parent\n",
        "    \n",
        "    # Create YOLOv8 config\n",
        "    config = {\n",
        "        'path': str(base_path.absolute()),\n",
        "        'train': 'train',\n",
        "        'val': 'val',\n",
        "        'names': {i: name for i, name in enumerate(class_names)},\n",
        "        'nc': len(class_names)\n",
        "    }\n",
        "    \n",
        "    with open(output_config, 'w') as f:\n",
        "        yaml.dump(config, f, default_flow_style=False)\n",
        "    \n",
        "    print(f\"Dataset config created: {output_config}\")\n",
        "    print(f\"Base path: {base_path}\")\n",
        "    print(f\"Classes ({len(class_names)}): {class_names}\")\n",
        "    return output_config\n",
        "\n",
        "config_file = prepare_yolo_config(TRAIN_DATASET, VAL_DATASET)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert COCO to YOLO format\n",
        "from src.train import convert_coco_to_yolo\n",
        "\n",
        "# Convert train set\n",
        "convert_coco_to_yolo(TRAIN_DATASET)\n",
        "\n",
        "# Convert val set  \n",
        "convert_coco_to_yolo(VAL_DATASET)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize model with pretrained weights\n",
        "if Path(PRETRAINED_WEIGHTS).exists():\n",
        "    print(f\"Loading pretrained weights from: {PRETRAINED_WEIGHTS}\")\n",
        "    model = YOLO(PRETRAINED_WEIGHTS)\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è Pretrained weights not found, using default YOLOv8{MODEL_SIZE}\")\n",
        "    model = YOLO(f\"yolov8{MODEL_SIZE}.pt\")\n",
        "\n",
        "# Train model\n",
        "# WandB is already initialized in Step 2, so it will automatically log training metrics\n",
        "results = model.train(\n",
        "    data=config_file,\n",
        "    epochs=EPOCHS,\n",
        "    imgsz=IMG_SIZE,\n",
        "    batch=BATCH,\n",
        "    project=\"/content/drive/MyDrive/credit_card_yolov12/models\",\n",
        "    name=f\"private_objects_{MODEL_SIZE}\",\n",
        "    exist_ok=True,\n",
        "    save=True,\n",
        "    plots=True,\n",
        "    val=True,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Save Model to Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# Paths\n",
        "MODEL_DIR = results.save_dir\n",
        "DRIVE_MODEL_DIR = f\"/content/drive/MyDrive/credit_card_yolov12/models/private_objects_{MODEL_SIZE}\"\n",
        "\n",
        "# Check if models are already in Drive (they are, since training saves directly to Drive)\n",
        "if str(MODEL_DIR) == DRIVE_MODEL_DIR:\n",
        "    print(f\"‚úÖ Models already saved to Drive: {MODEL_DIR}\")\n",
        "    print(f\"   Best model: {Path(MODEL_DIR) / 'weights' / 'best.pt'}\")\n",
        "    print(f\"   Last model: {Path(MODEL_DIR) / 'weights' / 'last.pt'}\")\n",
        "    DRIVE_MODEL_DIR = MODEL_DIR  # Use the same directory\n",
        "else:\n",
        "    # Copy best model to Drive\n",
        "    best_model = Path(MODEL_DIR) / \"weights\" / \"best.pt\"\n",
        "    last_model = Path(MODEL_DIR) / \"weights\" / \"last.pt\"\n",
        "    \n",
        "    Path(DRIVE_MODEL_DIR).mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    if best_model.exists():\n",
        "        shutil.copy2(best_model, f\"{DRIVE_MODEL_DIR}/best.pt\")\n",
        "        print(f\"‚úÖ Best model saved to: {DRIVE_MODEL_DIR}/best.pt\")\n",
        "    \n",
        "    if last_model.exists():\n",
        "        shutil.copy2(last_model, f\"{DRIVE_MODEL_DIR}/last.pt\")\n",
        "        print(f\"‚úÖ Last model saved to: {DRIVE_MODEL_DIR}/last.pt\")\n",
        "\n",
        "# Set DRIVE_MODEL_DIR for use in next cells\n",
        "print(f\"\\n‚úÖ Model directory: {DRIVE_MODEL_DIR}\")\n",
        "print(f\"‚úÖ Best model path: {Path(DRIVE_MODEL_DIR) / 'weights' / 'best.pt'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Evaluate on Progressive Occlusion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on progressive occlusion test sets\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Ensure we're in the repo directory\n",
        "REPO_DIR = \"/content/credit_card_yolov12\"\n",
        "os.chdir(REPO_DIR)\n",
        "sys.path.insert(0, REPO_DIR)\n",
        "\n",
        "# Find the actual split directory\n",
        "DATASET_BASE = \"/content/drive/MyDrive/credit_card_yolov12/datasets\"\n",
        "split_dirs = list(Path(DATASET_BASE).glob(\"*_split\"))\n",
        "if split_dirs:\n",
        "    TEST_SETS_BASE = str(split_dirs[0])\n",
        "    print(f\"‚úÖ Found split directory: {TEST_SETS_BASE}\")\n",
        "else:\n",
        "    TEST_SETS_BASE = f\"{DATASET_BASE}/private_dataset_split\"\n",
        "    print(f\"‚ö†Ô∏è Using fallback: {TEST_SETS_BASE}\")\n",
        "\n",
        "# Verify test occlusion directories exist\n",
        "print(f\"\\nChecking test occlusion directories:\")\n",
        "for level in [0, 25, 50, 75]:\n",
        "    test_dir = Path(TEST_SETS_BASE) / f\"test_occlusion_{level}\"\n",
        "    if test_dir.exists():\n",
        "        ann_file = test_dir / \"train\" / \"_annotations.coco.json\"\n",
        "        if ann_file.exists():\n",
        "            print(f\"  ‚úÖ test_occlusion_{level}: {ann_file}\")\n",
        "        else:\n",
        "            print(f\"  ‚ö†Ô∏è test_occlusion_{level}: exists but no annotation file\")\n",
        "    else:\n",
        "        print(f\"  ‚ùå test_occlusion_{level}: not found\")\n",
        "\n",
        "BEST_MODEL = f\"{DRIVE_MODEL_DIR}/weights/best.pt\"\n",
        "print(f\"\\nBest model: {BEST_MODEL}\")\n",
        "print(f\"Test sets base: {TEST_SETS_BASE}\")\n",
        "print(f\"Best model exists: {Path(BEST_MODEL).exists()}\")\n",
        "\n",
        "# Verify script exists\n",
        "script_path = Path(REPO_DIR) / \"src\" / \"evaluate_progressive.py\"\n",
        "if script_path.exists():\n",
        "    print(f\"‚úÖ Found script: {script_path}\")\n",
        "    !python src/evaluate_progressive.py \\\n",
        "        --model {BEST_MODEL} \\\n",
        "        --test-sets {TEST_SETS_BASE} \\\n",
        "        --output /content/drive/MyDrive/credit_card_yolov12/outputs/progressive_evaluation_private\n",
        "else:\n",
        "    print(f\"‚ùå Script not found at: {script_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Log Results to WandB\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Load progressive evaluation results\n",
        "results_file = \"/content/drive/MyDrive/credit_card_yolov12/outputs/progressive_evaluation_private/progressive_results.json\"\n",
        "\n",
        "if Path(results_file).exists():\n",
        "    with open(results_file, 'r') as f:\n",
        "        eval_results = json.load(f)\n",
        "    \n",
        "    # Log to wandb\n",
        "    for occlusion_level, metrics in eval_results.items():\n",
        "        wandb.log({\n",
        "            f\"mAP50_occlusion_{occlusion_level}\": metrics['mAP50'],\n",
        "            f\"mAP50_95_occlusion_{occlusion_level}\": metrics['mAP50_95'],\n",
        "            f\"precision_occlusion_{occlusion_level}\": metrics['precision'],\n",
        "            f\"recall_occlusion_{occlusion_level}\": metrics['recall'],\n",
        "            f\"f1_occlusion_{occlusion_level}\": metrics['f1']\n",
        "        })\n",
        "    \n",
        "    # Log visualization\n",
        "    plot_file = \"/content/drive/MyDrive/credit_card_yolov12/outputs/progressive_evaluation_private/progressive_occlusion_results.png\"\n",
        "    if Path(plot_file).exists():\n",
        "        wandb.log({\"progressive_occlusion_plot\": wandb.Image(plot_file)})\n",
        "    \n",
        "    print(\"‚úÖ Results logged to Weights & Biases\")\n",
        "    print(\"\\nProgressive Occlusion Results:\")\n",
        "    df = pd.DataFrame(eval_results).T\n",
        "    print(df)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Results file not found\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Finish wandb run\n",
        "wandb.finish()\n",
        "print(\"‚úÖ Training complete! Check your Weights & Biases dashboard.\")\n",
        "print(f\"‚úÖ Model saved to: {DRIVE_MODEL_DIR}\")\n",
        "print(f\"‚úÖ Results saved to: /content/drive/MyDrive/credit_card_yolov12/outputs/progressive_evaluation_private\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
